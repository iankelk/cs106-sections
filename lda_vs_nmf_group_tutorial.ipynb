{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1fac05bd",
      "metadata": {
        "id": "1fac05bd"
      },
      "source": [
        "# Topic Modeling Tutorial: LDA vs NMF (20 Newsgroups)\n",
        "\n",
        "This notebook walks you through a **clean, fair comparison** of two popular topic modeling methods:\n",
        "\n",
        "- **Latent Dirichlet Allocation (LDA)** — a **probabilistic** model that learns topics as word distributions and documents as mixtures of topics.\n",
        "- **Non‑negative Matrix Factorization (NMF)** — a **linear algebra** approach that factorizes a document–term matrix into non‑negative parts (“topics”) and their document weights.\n",
        "\n",
        "### What you will learn\n",
        "1. How to build a **shared vocabulary** to make comparisons fair.\n",
        "2. Why LDA uses **raw counts** and NMF prefers **TF‑IDF**.\n",
        "3. How to examine **top words** per topic.\n",
        "4. How to compute **two model‑agnostic quality metrics**:\n",
        "   - **UMass coherence** (higher/less negative ≈ better)\n",
        "   - **Mean Jensen–Shannon distance (JSD)** between topics (higher = more distinct)\n",
        "5. How to interpret results and common trade‑offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a828b16",
      "metadata": {
        "id": "5a828b16"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# For this demo we accept \"good enough\" convergence if top words are stable (which they are as you'll see)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cbc0b95",
      "metadata": {
        "id": "0cbc0b95"
      },
      "source": [
        "## Step 1 — Configure the experiment\n",
        "\n",
        "We set a few knobs to keep the run fast and reproducible while still informative:\n",
        "\n",
        "- `n_samples`: how many documents we’ll use from the dataset.\n",
        "- `n_topics`: how many topics to learn (a modeling choice; we’ll use 10).\n",
        "- `n_top_words`: how many representative words to display per topic.\n",
        "- `max_features`: cap the vocabulary size to control noise and runtime.\n",
        "- `random_state`: fixed seed for repeatable results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b5ba8d",
      "metadata": {
        "id": "a3b5ba8d"
      },
      "outputs": [],
      "source": [
        "n_samples    = 2000      # subset for speed\n",
        "n_topics     = 10        # number of topics to learn\n",
        "n_top_words  = 20        # words to display per topic\n",
        "max_features = 5000      # vocabulary cap\n",
        "random_state = 0         # reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad847dc",
      "metadata": {
        "id": "5ad847dc"
      },
      "source": [
        "## Step 2 — Load the dataset\n",
        "\n",
        "We’ll use the **20 Newsgroups** corpus (a classic text dataset).  \n",
        "To focus on message bodies (not metadata), we remove **headers, footers, and quoted text**.\n",
        "\n",
        "> Outcome of this step: a list of raw documents (`docs`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743cad0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "743cad0d",
        "outputId": "d45b63d9-2d1a-48d8-c094-0d2cb0a92fda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "ds = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                        remove=('headers', 'footers', 'quotes'))\n",
        "docs = ds.data[:n_samples]\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a9396c",
      "metadata": {
        "id": "83a9396c"
      },
      "source": [
        "## Step 3 — Build a **shared vocabulary** (fair comparison)\n",
        "\n",
        "To compare LDA and NMF fairly, both must see **the same words**. We therefore:\n",
        "\n",
        "1. Fit **one** `CountVectorizer` with identical rules (stop words, `min_df`, `max_df`, token pattern, and bigrams).\n",
        "2. Use the resulting **counts matrix** (`X_counts`) as input to **LDA**.\n",
        "3. Transform those same counts to **TF‑IDF** (`tfidf`) as input to **NMF**.\n",
        "\n",
        "**Why these choices?**\n",
        "- LDA’s generative assumptions expect **counts**.\n",
        "- NMF typically behaves better on **TF‑IDF** (downweights ubiquitous words).\n",
        "\n",
        "**Tokenizer settings**\n",
        "- `token_pattern` drops numbers and 1‑character tokens.\n",
        "- `ngram_range=(1,2)` includes both unigrams and bigrams for more context.\n",
        "- `min_df` / `max_df` remove very rare and very common terms to reduce noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d8ceb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2d8ceb7",
        "outputId": "49db0f82-06fd-4818-840a-f8c0d890334c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 2513), (2000, 2513), 2513)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "cv = CountVectorizer(\n",
        "    max_df=0.5, min_df=10, stop_words='english',\n",
        "    token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',  # drop numbers / 1-char tokens\n",
        "    ngram_range=(1, 2), max_features=max_features\n",
        ")\n",
        "X_counts = cv.fit_transform(docs)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "tfidf = TfidfTransformer(norm='l2', sublinear_tf=True).fit_transform(X_counts)\n",
        "\n",
        "X_counts.shape, tfidf.shape, len(feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620b70fe",
      "metadata": {
        "id": "620b70fe"
      },
      "source": [
        "## Step 4 — Fit LDA (counts) and NMF (TF‑IDF)\n",
        "\n",
        "### LDA (Latent Dirichlet Allocation)\n",
        "- **Input**: `X_counts` (document–term counts).\n",
        "- **Intuition**: each topic is a probability distribution over words; each document mixes topics with certain proportions.\n",
        "- **Key params**:\n",
        "  - `n_components=n_topics`: number of topics to discover.\n",
        "  - `learning_method='online'`: efficient variational optimization.\n",
        "  - `max_iter=10`: good enough for demos; more iterations refine topics.\n",
        "\n",
        "### NMF (Non‑negative Matrix Factorization)\n",
        "- **Input**: `tfidf` (document–term TF‑IDF weights).\n",
        "- **Intuition**: factorize the matrix into **W (docs×topics)** and **H (topics×terms)** with non‑negative entries; rows of **H** act like topics.\n",
        "- **Key params**:\n",
        "  - `init='nndsvd'`: structured initialization that speeds convergence.\n",
        "  - `solver='cd'`, `beta_loss='frobenius'`: coordinate descent on squared error.\n",
        "  - `alpha_W=alpha_H=0.5`, `l1_ratio=1.0`: stronger L1 sparsity for **sharper, more distinct** topics.\n",
        "  - `tol=1e-1`: “good‑enough” stopping; avoids chasing tiny changes.\n",
        "\n",
        "**Loss choice (`beta_loss`)**\n",
        "We set `beta_loss='frobenius'`, which makes NMF minimize squared error $|X - WH|_F^2$ (Frobenius norm).\n",
        "\n",
        "* Works well with **TF-IDF** (real-valued, nonnegative features).\n",
        "* Compatible with the fast coordinate-descent solver.\n",
        "* Tends to produce clean, sparse “parts” when combined with L1 regularization (`l1_ratio=1.0`, `alpha_W/alpha_H`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f653a9",
      "metadata": {
        "id": "11f653a9"
      },
      "outputs": [],
      "source": [
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics, max_iter=10,\n",
        "    learning_method='online', learning_offset=50.,\n",
        "    random_state=random_state\n",
        ").fit(X_counts)\n",
        "\n",
        "nmf = NMF(\n",
        "    n_components=n_topics, init='nndsvd', solver='cd',\n",
        "    beta_loss='frobenius',\n",
        "    alpha_W=0.5, alpha_H=0.5, l1_ratio=1.0,  # strong L1 ⇒ sparse, distinct topics\n",
        "    max_iter=2000, tol=1e-1,\n",
        "    random_state=random_state\n",
        ").fit(tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "030ed2f2",
      "metadata": {
        "id": "030ed2f2"
      },
      "source": [
        "## Step 5 — Helper functions\n",
        "\n",
        "We’ll need three utilities:\n",
        "\n",
        "1. **`print_top_words`** — show the top‑`N` words for each topic.\n",
        "2. **`umass_coherence`** — a quick **coherence** estimator using same‑corpus co‑occurrence:  \n",
        "   $$ C_{UMass}(T) = \\frac{1}{|P|} \\sum_{(w_i,w_j)\\in P} \\log \\frac{D(w_i, w_j) + \\epsilon}{D(w_j) + \\epsilon} $$\n",
        "   where $D(w_i, w_j)$ counts documents containing both words, and $P$ iterates over pairs from a topic’s top words.\n",
        "3. **`mean_js_distance`** — average **Jensen–Shannon divergence** between every pair of topic word distributions (higher = more distinct).\n",
        "\n",
        "## UMass coherence\n",
        "\n",
        "**What it asks:** *Do a topic’s top words tend to appear in the same documents?*\n",
        "\n",
        "For each topic, take its top words and look up how often each pair co-occurs across documents in your corpus. Aggregate those pairwise signals into one score.\n",
        "\n",
        "**How to read it:** Values are often **negative** (because of logarithms); **closer to 0 is better**. Higher/less-negative means the top words for a topic are found together more frequently, which usually aligns with human interpretability.\n",
        "\n",
        "**Strengths/limits:** Fast and corpus-specific (a good thing), but sensitive to preprocessing and very common/rare words. It measures *internal consistency*, not uniqueness from other topics.\n",
        "\n",
        "## Mean Jensen–Shannon distance (JSD) between topics\n",
        "\n",
        "**What it asks:** *How different are the topics from each other?*\n",
        "\n",
        "Treat each topic as a probability distribution over terms. Compute the Jensen–Shannon divergence for every pair of topics and average.\n",
        "\n",
        "**How to read it:** **Higher means more distinct** topics (less vocabulary overlap). With natural logs, JSD ranges from **0** (identical) to about **0.69** (maximally different).\n",
        "\n",
        "**Strengths/limits:** Captures *separation* between topics. It does **not** tell you if a topic is meaningful, only that topics differ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37935fa0",
      "metadata": {
        "id": "37935fa0"
      },
      "outputs": [],
      "source": [
        "def print_top_words(model, feats, topn=20, title=\"Topics\"):\n",
        "    print(f\"\\n{title}:\")\n",
        "    for k, comp in enumerate(model.components_):\n",
        "        top = np.argsort(comp)[-topn:][::-1]\n",
        "        print(f\"Topic #{k}:\\n\" + \" \".join(feats[top]))\n",
        "\n",
        "def _topn_indices(components, topn=20):\n",
        "    return [np.argsort(c)[-topn:][::-1] for c in components]\n",
        "\n",
        "def umass_coherence(components, X_counts, topn=20, eps=1e-12):\n",
        "    # Binarize in a safe dtype (avoid int8 overflow)\n",
        "    Xb = (X_counts > 0).astype(np.int32)\n",
        "    # Co-doc counts in int64 to be safe\n",
        "    C = (Xb.T @ Xb).astype(np.int64).tocsr()\n",
        "    df = np.asarray(Xb.sum(axis=0)).ravel().astype(np.int64)\n",
        "\n",
        "    topics = _topn_indices(components, topn)\n",
        "    vals = []\n",
        "    for ids in topics:\n",
        "        s = 0.0; pairs = 0\n",
        "        for i in range(1, len(ids)):\n",
        "            wi = ids[i]\n",
        "            for j in range(i):\n",
        "                wj = ids[j]\n",
        "                co = C[wi, wj]      # scalar int64\n",
        "                denom = df[wj]\n",
        "                s += np.log((co + eps) / (denom + eps))\n",
        "                pairs += 1\n",
        "        vals.append(s / max(pairs, 1))\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def mean_js_distance(components, eps=1e-12):\n",
        "    # Topic separation: mean pairwise Jensen–Shannon divergence between topics\n",
        "    P = components / (components.sum(axis=1, keepdims=True) + eps)\n",
        "    def js(p, q):\n",
        "        m = 0.5*(p+q)\n",
        "        def kl(a, b):\n",
        "            a = a + eps; b = b + eps\n",
        "            return float((a*np.log(a/b)).sum())\n",
        "        return 0.5*kl(p, m) + 0.5*kl(q, m)\n",
        "    pairs = [js(P[i], P[j]) for i, j in combinations(range(P.shape[0]), 2)]\n",
        "    return float(np.mean(pairs)) if pairs else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a23133",
      "metadata": {
        "id": "e0a23133"
      },
      "source": [
        "## Step 6 — Inspect the learned topics\n",
        "\n",
        "We print the **top words** per topic for each model:\n",
        "\n",
        "- **LDA**: words with the highest probability $p(w\\mid z)$ for topic $z$.\n",
        "- **NMF**: words with the largest weights in the topic’s component (the “parts” with strongest contribution).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad7656c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ad7656c",
        "outputId": "5ba8ca5c-2764-4115-aa12-5f46dc454f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topics in LDA (counts):\n",
            "Topic #0:\n",
            "window food ago ve printer display just new hp win monitor years ago like using problem postscript font true fonts times\n",
            "Topic #1:\n",
            "drive disk card drives hard scsi controller speed rom bios hard disk floppy feature board supports interface bus mb pin bit\n",
            "Topic #2:\n",
            "banks gordon edu surrender pitt gordon banks intellect soon cadre skepticism shameful pitt edu geb dsl pitt dsl chastity chastity intellect edu shameful geb cadre shameful surrender\n",
            "Topic #3:\n",
            "people just don think like know god time good way say does make really right did didn want things going\n",
            "Topic #4:\n",
            "use key new like space know thanks used chip need work just does problem don power encryption keys clipper good\n",
            "Topic #5:\n",
            "cache ex meg powers simms ram set tv price guide high want problem output happening rest cycle normal turn operation\n",
            "Topic #6:\n",
            "edu windows mail com file graphics send version ftp available program software files contact pub pc image use server time\n",
            "Topic #7:\n",
            "people israel said armenians armenian killed children greek turkish jews dead men women world did jewish war time attacks israeli\n",
            "Topic #8:\n",
            "copies helmet double copy dod radio games year paint smith pre print star student edu left tx cable org good\n",
            "Topic #9:\n",
            "government period law new president health section year play team flyers information division american national gm vs crime pp military\n",
            "\n",
            "Topics in NMF (TF-IDF, Frobenius):\n",
            "Topic #0:\n",
            "just like don know people think does good time use new way make want thanks need ve really say right\n",
            "Topic #1:\n",
            "thanks windows mail file card advance hi edu thanks advance drive software help email dos pc ftp info using video version\n",
            "Topic #2:\n",
            "edu geb gordon banks pitt edu cadre shameful surrender geb cadre skepticism chastity chastity intellect intellect geb chastity banks skepticism dsl dsl pitt edu shameful cadre dsl surrender soon pitt gordon skepticism\n",
            "Topic #3:\n",
            "does god thanks know advance thanks advance jesus mail does know bible christians hi christian people anybody faith true christ religion info\n",
            "Topic #4:\n",
            "thanks game know team thanks advance advance games mail year car like don interested season play bike just ll good win\n",
            "Topic #5:\n",
            "thanks chip government mail interested key edu information clipper encryption com advance email thanks advance keys address phone list public security\n",
            "Topic #6:\n",
            "edu mail god game team year com university file games ftp new send list play book space address win internet\n",
            "Topic #7:\n",
            "drive god new sale hard drives power jesus floppy offer hard drive card disk price condition shipping software faith bible christian\n",
            "Topic #8:\n",
            "game team drive chip does games play card win key baseball clipper players software season hard data advance drives think\n",
            "Topic #9:\n",
            "don edu think file know drive don know com need send ftp internet want mail people wrong chip pub card don think\n"
          ]
        }
      ],
      "source": [
        "print_top_words(lda, feature_names, n_top_words, \"Topics in LDA (counts)\")\n",
        "print_top_words(nmf, feature_names, n_top_words, \"Topics in NMF (TF-IDF, Frobenius)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's estimate the topics!\n",
        "\n",
        "* **#0 — Windows/printing & fonts**\n",
        "  *window, printer, hp, postscript, font, monitor, display*\n",
        "* **#1 — PC hardware: disks/SCSI/controllers**\n",
        "  *drive, disk, scsi, controller, bios, floppy, board, bus*\n",
        "* **#2 — Usenet signature/meme cluster (“Gordon Banks / pitt / geb / cadre”)**\n",
        "  Artifacty block from repeated sig lines & threads.\n",
        "* **#3 — General debate / opinion language**\n",
        "  *people, just, don, think, know, time, right, want*\n",
        "* **#4 — Crypto & Clipper chip / encryption**\n",
        "  *key, encryption, clipper, keys, chip, power, use*\n",
        "* **#5 — Performance & memory tuning**\n",
        "  *cache, ram, simms, cycle, operation, output, set*\n",
        "* **#6 — Software distribution & FTP**\n",
        "  *windows, file, ftp, version, program, server, pc, image*\n",
        "* **#7 — Geopolitics / Middle East & Armenia**\n",
        "  *israel, armenian, greek, turkish, jews, war, israeli*\n",
        "* **#8 — Motorcycles / DoD in-jokes / gear**\n",
        "  *helmet, dod, radio, paint, cable, tx, star, copy*\n",
        "* **#9 — Sports (NHL—Flyers) with a dash of civic/news terms**\n",
        "  *team, flyers, division, play, gm vs* (+ a few government-ish strays)\n",
        "\n",
        "# NMF (TF-IDF, Frobenius) topic labels (estimated)\n",
        "\n",
        "* **#0 — General conversational filler / opinions**\n",
        "  *just, like, don, know, think, good, way, make*\n",
        "* **#1 — Windows & help/FTP/email**\n",
        "  *windows, mail, file, card, drive, ftp, dos, version*\n",
        "* **#2 — Usenet signature/meme cluster (pitt/geb/Gordon Banks)**\n",
        "  Same artifact as LDA #2.\n",
        "* **#3 — Religion/Christianity Q&A**\n",
        "  *god, jesus, bible, christian*, plus “thanks/advance” email tone.\n",
        "* **#4 — Sports & hobby chatter (hockey/baseball/bike/car)**\n",
        "  *game, team, season, play, win, car, bike*\n",
        "* **#5 — Crypto policy / Clipper & keys**\n",
        "  *chip, government, key, encryption, clipper, public, security*\n",
        "* **#6 — General internet/edu/com, lists & announcements**\n",
        "  *edu, com, university, ftp, list, internet, space*\n",
        "* **#7 — Hardware buy/sell & upgrades (with some religious bleed)**\n",
        "  *drive, sale, hard, price, shipping, floppy* (plus stray *jesus, bible*)\n",
        "* **#8 — Sports + computing/crypto overlap**\n",
        "  *game, team, card, baseball, players, chip, clipper, drives*\n",
        "* **#9 — Helpdesk / FTP how-to & “I need…” posts**\n",
        "  *don, think, file, drive, ftp, internet, mail, send*"
      ],
      "metadata": {
        "id": "eE2L0c3bPhlu"
      },
      "id": "eE2L0c3bPhlu"
    },
    {
      "cell_type": "markdown",
      "id": "31d4d174",
      "metadata": {
        "id": "31d4d174"
      },
      "source": [
        "## Step 7 — Evaluate with model‑agnostic metrics\n",
        "\n",
        "We summarize each model with two numbers:\n",
        "\n",
        "1. **UMass coherence**: how often the top words of a topic co‑occur in documents.  \n",
        "   Higher (less negative) generally means **more interpretable** topics.\n",
        "2. **Mean JSD separation**: how far apart the topic distributions are from each other.  \n",
        "   Higher means **more distinct** topics (less overlap).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b775078f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b775078f",
        "outputId": "673fe311-9b58-436a-82a3-30f68215c292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Summary Metrics ===\n",
            "LDA  coherence (UMass): -4.3433   | separation (JSD): 0.4010\n",
            "NMF  coherence (UMass): -4.4337   | separation (JSD): 0.4413\n"
          ]
        }
      ],
      "source": [
        "lda_coh = umass_coherence(lda.components_, X_counts, topn=n_top_words)\n",
        "nmf_coh = umass_coherence(nmf.components_, X_counts, topn=n_top_words)\n",
        "\n",
        "lda_js  = mean_js_distance(lda.components_)\n",
        "nmf_js  = mean_js_distance(nmf.components_)\n",
        "\n",
        "print(\"\\n=== Summary Metrics ===\")\n",
        "print(f\"LDA  coherence (UMass): {lda_coh:.4f}   | separation (JSD): {lda_js:.4f}\")\n",
        "print(f\"NMF  coherence (UMass): {nmf_coh:.4f}   | separation (JSD): {nmf_js:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76cf450a",
      "metadata": {
        "id": "76cf450a"
      },
      "source": [
        "## Step 8 — Interpret the results\n",
        "\n",
        "Typical patterns you may observe:\n",
        "\n",
        "- **Coherence**: LDA often edges NMF because its probabilistic assumptions favor word co‑occurrence within topics.\n",
        "- **Separation (JSD)**: With strong sparsity, NMF topics can be more distinct (higher JSD).\n",
        "- **Trade‑offs**: LDA topics may look slightly more diffuse but coherent; NMF topics can be sharper and easier to label, especially with TF‑IDF.\n",
        "\n",
        "### Practical tips\n",
        "- If topics look redundant, try adjusting `n_topics`, `min_df`, or `max_df`.\n",
        "- If NMF warns about convergence but top words stabilize, it’s fine for interpretation.\n",
        "- Keep the **shared vocabulary** so your comparison remains fair.\n",
        "\n",
        "### Let's see the specifics here\n",
        "\n",
        "* **Coherence (UMass)**: LDA −4.3433 vs NMF −4.4337: basically a tie, with a tiny edge to LDA (less negative = better). This matches intuition: LDA often groups co-occurring words a tad more cleanly.\n",
        "* **Separation (JSD)**: LDA 0.4010 vs NMF 0.4413: NMF topics are **more distinct** from one another. That strong L1 sparsity did its job.\n",
        "\n",
        "### What L1 sparsity does here\n",
        "\n",
        "* **Zeros-out many weights.** Most entries in (W) and (H) become exactly 0.\n",
        "* **Sharper topics.** Each topic uses a **small, distinctive** set of words → less vocabulary overlap between topics.\n",
        "* **Sparser docs.** Each document loads on **fewer topics**.\n",
        "* **Higher separation.** Because topics share fewer words, the **mean JSD** between topics goes up (we observed NMF JSD 0.4413 > LDA 0.4010).\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "* **Slower/never “fully” converging.** Heavy L1 makes coordinate descent keep “nibbling,” hence I had a ton of ConvergenceWarnings. (The topics still looked stable, so results were fine.)\n",
        "* **Slightly lower coherence sometimes.** Strong sparsity can drop a few supportive words, so UMass may be a bit worse than LDA (as we saw: very close, slight LDA edge)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8587ac06",
      "metadata": {
        "id": "8587ac06"
      },
      "source": [
        "Step 9 — Check topic stability\n",
        "\n",
        "To demonstrate robustness, fit NMF twice with different random seeds and compare the **overlap of top‑20 words** per topic. High overlap means stable topics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e1cfb18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1cfb18",
        "outputId": "28db6fbd-1581-4ad2-c07b-7626f5afc731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMF topic stability (avg top-20 overlap): 0.9350000000000002\n"
          ]
        }
      ],
      "source": [
        "nmf_a = NMF(\n",
        "    n_components=n_topics, init='nndsvd', solver='cd',\n",
        "    beta_loss='frobenius',\n",
        "    alpha_W=0.5, alpha_H=0.5, l1_ratio=1.0,\n",
        "    max_iter=800, tol=1e-2, random_state=0\n",
        ").fit(tfidf)\n",
        "\n",
        "nmf_b = NMF(\n",
        "    n_components=n_topics, init='nndsvd', solver='cd',\n",
        "    beta_loss='frobenius',\n",
        "    alpha_W=0.5, alpha_H=0.5, l1_ratio=1.0,\n",
        "    max_iter=800, tol=1e-2, random_state=1\n",
        ").fit(tfidf)\n",
        "\n",
        "def avg_top_overlap(A, B, topn=20):\n",
        "    overlaps = []\n",
        "    for i in range(A.components_.shape[0]):\n",
        "        ta = set(np.argsort(A.components_[i])[-topn:])\n",
        "        tb = set(np.argsort(B.components_[i])[-topn:])\n",
        "        overlaps.append(len(ta & tb) / topn)\n",
        "    return float(np.mean(overlaps))\n",
        "\n",
        "print(\"NMF topic stability (avg top-20 overlap):\", avg_top_overlap(nmf_a, nmf_b))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **NMF stability**: **0.935** average top-20 overlap across restarts is excellent. The NMF topics are highly stable."
      ],
      "metadata": {
        "id": "4-HAGhtARR4z"
      },
      "id": "4-HAGhtARR4z"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}